{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d715863c",
   "metadata": {},
   "source": [
    "### <a name=\"index\"></a>Index\n",
    "\n",
    "### [Qué es Airflow?](#mark_01)\n",
    "\n",
    "### [Qué significa DAG?](#mark_02)\n",
    "\n",
    "### [Task y Operators?](#mark_03)\n",
    "\n",
    "### [Scheduler](#mark_04)\n",
    "\n",
    "### [Instalación](#mark_05)\n",
    "\n",
    "### [Configuración](#mark_06)\n",
    "\n",
    "### [Dónde está el fichero de configuración?](#mark_07)\n",
    "\n",
    "### [Variables y conexiones](#mark_08)\n",
    "\n",
    "### [Creando Dags](#mark_09)\n",
    "\n",
    "### [Creando Tasks con Bash Operator](#mark_10)\n",
    "\n",
    "### [Creando Tasks con Python Operator](#mark_11)\n",
    "\n",
    "### [Definiendo dependencias entre tareas](#mark_12)\n",
    "\n",
    "### [Custom Operator](#mark_13)\n",
    "\n",
    "### [Orquestando un DAG I, Orquestar y monitorizar procesos](#mark_14)\n",
    "\n",
    "### [Orquestando un DAG II, cron](#mark_15)\n",
    "\n",
    "### [Monitoring - Fallos](#mark_16)\n",
    "\n",
    "### [Task Actions, \"Clean\", \"Mark Failed\", y \"Mark Success\"](#mark_17)\n",
    "\n",
    "### [Trigger Rules](#mark_18)\n",
    "\n",
    "### [Que son los sensores?](#mark_19)\n",
    "\n",
    "   ### [External Task Sensor](#mark_20)\n",
    "\n",
    "   ### [File Sensor](#mark_21)\n",
    "\n",
    "### [Qué son los templates con Jinja?](#mark_22)\n",
    "\n",
    "### [XComs - cross comunications](#mark_23)\n",
    "\n",
    "### [Branch Python Operator](#mark_24)\n",
    "\n",
    "### [](#mark_)\n",
    "\n",
    "### [](#mark_)\n",
    "\n",
    "### [](#mark_)\n",
    ".................................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5816ac5",
   "metadata": {},
   "source": [
    "# <a name=\"mark_01\"></a>Qué es Airflow?\n",
    "[Index](#index)\n",
    "\n",
    "Airflow es un orquestador, asi que se encarga de decir como y cuando hacer las cosas, pero otros deben ser los que ejecuten.\n",
    "\n",
    "Airflow es como el señor que tiene la batuta en una orquesta, el nunca toca un instrumento pero es el que dirije a toda la gente para que haga mejor en lo que cada uno es experto.\n",
    "![](img_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85aa47",
   "metadata": {},
   "source": [
    "# <a name=\"mark_02\"></a>Qué significa DAG?\n",
    "[Index](#index)\n",
    "\n",
    "DAG( Directed Acyclic Graph)\n",
    "\n",
    "Los Workflow los Podemos realizar a través de los DAG. Viene del concepto de los grafos y debe tener 2 cualidades.\n",
    "En primer lugar una dirección. Todas las aristas fluyen hacia una dirección.\n",
    "La segunda cualidad es que no pueden tener ciclos. Una arista que sale de un nodo no puede volver a ese nodo.\n",
    "\n",
    "![](img_02.png)\n",
    "![](img_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13255714",
   "metadata": {},
   "source": [
    "# <a name=\"mark_03\"></a>Task y Operators?\n",
    "[Index](#index)\n",
    "\n",
    "\n",
    "Un DAG está compuesto por tareas:\n",
    "![](img_04.png)\n",
    "\n",
    "- Operators.\n",
    "- Sensors.\n",
    "- TaskFlow-decorated@task.\n",
    "![](img_05.png)\n",
    "\n",
    "Las tareas son creadas a través de los \"operators\".\n",
    "\n",
    "Entre los operators podemos utilizar( entre otros) BashOperators o PythonOperators, algunas veces se pueden utilizar ambos, como demuestra el siguiente ejemplo.\n",
    "![](img_06.png)\n",
    "\n",
    "Nota: También se pueden implementar \"Operators\" propios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582066c3",
   "metadata": {},
   "source": [
    "# <a name=\"mark_04\"></a>Scheduler.\n",
    "[Index](#index)\n",
    "\n",
    "El \"scheduler\" es el corazón de Airflow.\n",
    "![](img_07.png)\n",
    "\n",
    "- Se conecta al directorio donde tenemos nuestros DAGs', allí están:\n",
    "    - Los ficheros Python con las descripción de las tareas\n",
    "    - dependencias.\n",
    "    - cuándo queremos ejecutar estos procesos.\n",
    "    \n",
    "- Todas estás tareas se otorgan a los \"Workers\"\n",
    "- El estado de estas tarea \"en ejecución\", \"finalizado\", etc. Se guarda en la Metadata Database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1983617",
   "metadata": {},
   "source": [
    "# <a name=\"mark_05\"></a>Instalación.\n",
    "[Index](#index)\n",
    "\n",
    "1. https://airflow.apache.org/\n",
    "2. Click en Install.\n",
    "3. [\"Quick Start\" --> \"How-to Guide -->\"Running Airflow in Docker\" --> \"Fetching docker-compose.yaml\"](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#fetching-docker-compose-yaml) \n",
    "\n",
    "Para este caso se describe el comando en Unix/Linux\n",
    "```bash\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.7.1/docker-compose.yaml'\n",
    "```\n",
    "Pero si lo queremos correr en Powershell (Windows), será:\n",
    "```bash\n",
    "Invoke-WebRequest -Uri \"https://airflow.apache.org/docs/apache-airflow/2.7.1/docker-compose.yaml\" -OutFile \"docker-compose.yaml\"\n",
    "```\n",
    "4. Luego ejecutar \"docker-compose up\" en terminal, para traer toda la arquitecura a mi proyecto.\n",
    "5. Ya podemos levantar el servicio en nuestro puerto 8080.\n",
    "    - user: airflow, pass: airflow\n",
    "![](img_08.png)\n",
    "![](img_09.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d98e79",
   "metadata": {},
   "source": [
    "# <a name=\"mark_06\"></a>Configuración.\n",
    "[Index](#index)\n",
    "\n",
    "![](img_10.png)\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html\n",
    "\n",
    "Luego vamos a setear \"el tiempo que tiene airflow para escanear los DAGs creados\".\n",
    "![](img_11.png)\n",
    "\n",
    "![](img_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef05280",
   "metadata": {},
   "source": [
    "# <a name=\"mark_07\"></a>Dónde está el fichero de configuración?\n",
    "[Index](#index)\n",
    "\n",
    "## Primer Forma:\n",
    "\n",
    "1. En la terminar correr el comando \"docker ps\" para ver los contenedores activos.\n",
    "\n",
    "2. Buscar el contenedor \"webserver\"\n",
    "![](img_13.png)\n",
    "\n",
    "3. Copiamos el \"container id\" y ejecutamos \"docker exec -it **container_id** bash\" (así entramos al contenedor)\n",
    "![](img_14.png)\n",
    "\n",
    "4. listamos los directorios con \"ls\", vemos que se encuentra el archivo \"airflow.cfg\"\n",
    "![](img_15.png)\n",
    "\n",
    "5. Mostramos su contenido con \"cat arirflow.cfg\"\n",
    "\n",
    "Con lo cual ya se pueden configurar en ese archivo.\n",
    "\n",
    "## Segunda Forma:\n",
    "\n",
    "1. Abrir el archivo \"docker-compose.yaml\"\n",
    "\n",
    "2. Buscamos la variable que queremos setear, como vimos anteriormente, para este ejemplo será \"AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL\".\n",
    "![](img_12.png)\n",
    "\n",
    "y la seteamos en el valor que deseamos.\n",
    "\n",
    "![](img_16.png)\n",
    "\n",
    "## Cambiar el Puerto:\n",
    "![](img_17.png)\n",
    "\n",
    "## Volumenes:\n",
    "![](img_18.png)\n",
    "\n",
    "Por ejemplo la línea \n",
    "```yaml\n",
    "- ./dags:/opt/airflow/dags\n",
    "```\n",
    "lo que nos permite es que todos los cambios realizados in mi carpeta local \"dags\" serán reflejados en la UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ef17e",
   "metadata": {},
   "source": [
    "# <a name=\"mark_08\"></a>Variables y conexiones\n",
    "[Index](#index)\n",
    "\n",
    "## Crear Variables:\n",
    "\n",
    "- En la UI, pestaña \"Admin\" --> \"variables\"\n",
    "![](img_19.png)\n",
    "\n",
    "![](img_20.png)\n",
    "\n",
    "## Llamando a la variable:\n",
    "\n",
    "![](img_21.png)\n",
    "\n",
    "## Crear conexiones:\n",
    "\n",
    "* Airflow posee una serie de Operadores/ Sensores por defectos que podemos utilizar. Alguno de estos Operadores nos solicitan una \"Cadena de Conexión\", Ejemplo \"hive_operator\".\n",
    "\n",
    "![](img_22.png)\n",
    "\n",
    "* Se solicita una conexión:\n",
    "![](img_23.png)\n",
    "\n",
    "* Esta conexión la podemos crear en Airflow.\n",
    "![](img_24.png)\n",
    "![](img_25.png)\n",
    "![](img_26.png)\n",
    "\n",
    "- Donde \"Connection id\" es el nombre de la conexión.\n",
    "- Connection Type, se elige de un desplegable y si no existe en la lista hay que agregarlo instalando el proveedor, por ejemplo...\n",
    "\n",
    "```python\n",
    "pip install apache-airflow-providers-mysql\n",
    "```\n",
    "\n",
    "- Para utilizar la conexión, reemplazando, en este caso, \"postgres_conn_id\" con el nombre de la conexión creada:\n",
    "\n",
    "```python\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "\n",
    "populate_pet_table = PostgresOperator(\n",
    "\t\ttask_id=\"populate_pet_table\",\n",
    "\t\tpostgres_conn_id=\"my_postgres_conn\",\n",
    "\t\tsql=\"sql/pet_schema.sql\",\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6105f2",
   "metadata": {},
   "source": [
    "# <a name=\"mark_09\"></a>Creando Dags:\n",
    "[Index](#index)\n",
    "\n",
    "## Primer forma usando \"Standar constructor\"\n",
    "\n",
    "![](img_27.png)\n",
    "\n",
    "## Segunda forma usando un Context manager.\n",
    "![](img_28.png)\n",
    "\n",
    "Gestores de contexto\n",
    "\n",
    "Tal vez nunca hayas oído hablar de los gestores de contexto o context managers, pero si has trabajado con ficheros ya los has usado sin darte cuenta. Si alguna vez has visto la cláusula with, todo lo que pasa por debajo hace uso de los gestores de contexto.\n",
    "\n",
    "Realmente no ofrecen ninguna funcionalidad nueva, pero permiten ahorrar código eliminando todo lo que sea repetitivo o boilerplate. En pocas palabras, permiten ejecutar dos tareas de manera automática, la primera al entrar al with y la segunda al salir del mismo.\n",
    "\n",
    "El ejemplo más típico es el siguiente. Abrimos un fichero, escribimos contenido en él, y lo cerramos.\n",
    "\n",
    "## Haciendo uso de los context managers\n",
    "```python\n",
    "with open('fichero.txt', 'w') as fichero:\n",
    "    fichero.write('Hola!')\n",
    "```\n",
    "### ¿Cómo que lo cerramos? \n",
    "\n",
    "Pues sí, aunque no se especifique expresamente, por debajo Python ejecutará la función close() cuando se salga del bloque with. Es importante notar también que la variable fichero no será accesible fuera del with, únciamente dentro.\n",
    "\n",
    "El siguiente código es totalmente equivalente al anterior, pero sin hacer uso de los context managers, simplemente de las excepciones.\n",
    "\n",
    "## Sin usar los context managers\n",
    "```python \n",
    "fichero = open('fichero.txt', 'w')\n",
    "try:\n",
    "    fichero.write('Hola!')\n",
    "finally:\n",
    "    fichero.close()\n",
    "```\n",
    "\n",
    "* Ejemplo:\n",
    "\n",
    "En nuestro proyecto creamos un archivo \".py\" con la declaración del primer dag.\n",
    "\n",
    "![](img_33.png)\n",
    "**Nota: Falta el \":\" después de \"as dag\" y la indentación de \"t1\"**\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"primer_dag\",\n",
    "    start_date=\"datetime(2023, 09, 01)\", #inicial el 1° de sept de 2023\n",
    "    shcedule_interval=\"@once\" #el dag se realiza una sola vez.\n",
    ") as dag:\n",
    "\n",
    "#Como aún no hemos visto operators, vamos a usar un dummy el cual no hace nada.\n",
    "    t1 = EmptyOperator(task_id=\"Dummy\")\n",
    "```\n",
    "\n",
    "## Tercer forma, usando un decorador.\n",
    "![](img_29.png)\n",
    "\n",
    "\n",
    "## Cómo se ve un Dag creado en la UI:\n",
    "![](img_30.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "![](img_31.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "![](img_32.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ea9fb",
   "metadata": {},
   "source": [
    "# <a name=\"mark_10\"></a>Creando Tasks con Bash Operator:\n",
    "[Index](#index)\n",
    "\n",
    "* Creamos un nuevo file dentro de \"dags\", para este ejemplo \"1-bash_operator.py\".\n",
    "![](img_34.png)\n",
    "**Nota: Falta el \":\" después de \"as dag\" y la indentación de \"t1\"**\n",
    "\n",
    "* Luego en la UI, podremos ejecutarlo con el switch \"Paues/Unpause DAG\"\n",
    "![](img_35.png)\n",
    "\n",
    "* El botón \"Auto-refresh\" permite refrescar la tarea automaticamente.\n",
    "![](img_36.png)\n",
    "\n",
    "* Haciendo click en el cuadradito corresopondiente a la \"task\", podemos acceder a \"log\".\n",
    "![](img_37.png)\n",
    "\n",
    "* En la parte final podemos ver el mensaje creado.\n",
    "![](img_38.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5ec60",
   "metadata": {},
   "source": [
    "# <a name=\"mark_11\"></a>Creando Tasks con Python Operator:\n",
    "[Index](#index)\n",
    "\n",
    "* La siguiente imagen representa la tarea creada con Python Operator en el proyecto local, La visualización en la UI es tal cual el ejemplo anterior.\n",
    "![](img_39.png)\n",
    "**Nota: Falta el \":\" después de \"as dag\" y la indentación de \"t1\"**\n",
    "\n",
    "* Script:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def print_hello():\n",
    "    print(\"hello a todos\")\n",
    "\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"python_operator\",\n",
    "    description=\"tarea implementado python operator\",\n",
    "    scheldule_interval=\"@once\",\n",
    "    start_date=datetime(2023, 9, 1)\n",
    ") as dag:\n",
    "\n",
    "    t1 = PythonOperator(\n",
    "        task_id=\"hello with python\"\n",
    "        python_callable= print_hello #acá se pasa la función de python a implementar\n",
    "        )\n",
    "    t1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff83806",
   "metadata": {},
   "source": [
    "# <a name=\"mark_12\"></a>Definiendo dependencias entre tareas:\n",
    "[Index](#index)\n",
    "\n",
    "* Que una tarea dependa de otra.\n",
    "\n",
    "## Utilizando \"set_downstream\" y el operador \"bitshift\" (>>)  \n",
    "![](img_40.png)\n",
    "\n",
    "## Dos tareas ejecutas al unisono en \"set_downstream\".\n",
    "![](img_41.png)\n",
    "\n",
    "## Dos tareas ejecutas al unisono en \"set_upstream\".\n",
    "![](img_42.png)\n",
    "\n",
    "## Ejemplo en código \"el archivo debe ser creado en el proyecto\":\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.dash import BashOperator\n",
    "\n",
    "def print_hello():\n",
    "    print(\"hello a todos\")\n",
    "\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"dependencias\",\n",
    "    description=\"creando dependencias entre tareas\",\n",
    "    scheldule_interval=\"@once\",\n",
    "    start_date=datetime(2023, 9, 1)\n",
    ") as dag:\n",
    "\n",
    "    t1 = PythonOperator(\n",
    "        task_id=\"tarea_01\"\n",
    "        python_callable= print_hello #acá se pasa la función de python a implementar\n",
    "        )\n",
    "    t2 = BashOperator(\n",
    "        task_id=\"tarea_02\",\n",
    "        bash_command=\"echo 'tarea_02'\"\n",
    "    )\n",
    "    t3 = BashOperator(\n",
    "        task_id=\"tarea_03\",\n",
    "        bash_command=\"echo 'tarea_03'\"\n",
    "    )\n",
    "    t4 = BashOperator(\n",
    "        task_id=\"tarea_04\",\n",
    "        bash_command=\"echo 'tarea_04'\"\n",
    "    )\n",
    "#Creando dependencias.\n",
    "\n",
    "t1.set_downstream(t2)\n",
    "t2.set_downstream([t3, t4])\n",
    "\n",
    "#lo mismo pero con bitshifts\n",
    "t1 >> t2 >> [t3, t4]\n",
    "```\n",
    "\n",
    "## Representación en Airflow UI.\n",
    "![](img_43.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa9da0",
   "metadata": {},
   "source": [
    "# <a name=\"mark_13\"></a>Custom Operator:\n",
    "[Index](#index)\n",
    "\n",
    "Creamos el archivo en el proyecto.\n",
    "![](img_44.png)\n",
    "**Nota: Falta el \":\" después de \"as dag\" y la indentación de \"t1\"**\n",
    "\n",
    "\n",
    "Pero también tenemos que crear la clase y el método que utilizaremos.\n",
    "![](img_45.png)\n",
    "\n",
    "Llamando a la Clase...listo para correr.\n",
    "![](img_46.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889ff22",
   "metadata": {},
   "source": [
    "# <a name=\"mark_14\"></a>Orquestando un DAG I, Orquestar y monitorizar procesos:\n",
    "[Index](#index)\n",
    "\n",
    "Documentación [https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html](https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html)\n",
    "\n",
    "![](img_47.png)\n",
    "\n",
    "- \"preset\" --> forma predifinida por Airflow para definir cada cuanto se deve ejecutar nuestros DAGs o procesos.\n",
    "- \"None\" --> No quiero que el proceso se ejecute, solo se ejecutará cuando otro DAG lo llame.\n",
    "- \"@once\" --> Ejecuta el proceso una sola vez.\n",
    "- \"@hourly\" --> Ejecuta el proceso cada hora.\n",
    "- y la misma lógica con los demás.\n",
    "\n",
    "### Si queremos utilizar una sintaxis más custom [https://crontab.guru/](https://crontab.guru/)\n",
    "\n",
    "\n",
    "valores de los 5 caracteres de cron\n",
    "\n",
    "![](img_48.png)\n",
    "\n",
    "Ejemplo.\n",
    "![](img_49.png)\n",
    "\n",
    "Tareas ejecutadas secuencialmente **default_args={\"depend_on_past\": True}** significa que si por cada día tenemos una tarea (t1,t2,t3,t4)\n",
    "la t1 del día siguiente, no se ejecutará si la t1 del día anterior, no\n",
    "se a ejecutado correctamente..\n",
    "![](img_50.png)\n",
    "\n",
    "Tareas ejecutadas secuencialmente **max_active_runs=1** significa que ejecuta todas las tareas de un día y luego sigue con las del día siguiente.\n",
    "![](img_51.png)\n",
    "\n",
    "### Finalmente el código debe quedar asi:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "'''Nuevo argumento \"depend_on_past\", argumento seteado en False por \n",
    "default, significa que si por cada día tenemos una tarea (t1,t2,t3,t4)\n",
    "la t1 del día siguiente, no se ejecutará si la t1 del día anterior, no\n",
    "se a ejecutado correctamente.'''\n",
    "with DAG(\n",
    "    dag_id=\"5.1-orquestacion\",\n",
    "    description=\"Pobando la orquestación\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2023, 9, 1),\n",
    "    end_date=datetime(2023, 10, 1),\n",
    "    default_args={\"depend_on_past\": True},\n",
    "    max_active_runs=1 #Ejecuta las tareas de un día y sigue con las del día siguiente. \n",
    ") as dag:\n",
    "    '''para este caso usamos el operador \"sleep 2\" para poder ver como \n",
    "    transicionan las tareas de un estado a otro, debido a que airflow\n",
    "    lo ejecutará muy rápido\n",
    "    \"&&\" --> Si sucede lo primero, realiza lo segundo, es un \"and\"'''\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"tarea_01\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_01'\"\n",
    "    )\n",
    "    t2 = BashOperator(\n",
    "        task_id=\"tarea_02\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_02'\"\n",
    "    )\n",
    "    t3 = BashOperator(\n",
    "        task_id=\"tarea_03\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_03'\"\n",
    "    )\n",
    "    t4 = BashOperator(\n",
    "        task_id=\"tarea_04\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_04'\"\n",
    "    )\n",
    "\n",
    "    t1 >> t2 >> [t3, t4]\n",
    "```\n",
    "![](img_52.png)\n",
    "\n",
    "## Nota: \n",
    "**Una buena manera de entenderlo también es con el scheduler interval @monthly, donde por ejemplo la ejecución del 1 de Febrero no se ejecuta hasta que sea el 1 de Marzo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd5b1a",
   "metadata": {},
   "source": [
    "# <a name=\"mark_15\"></a>Orquestando un DAG II, cron:\n",
    "[Index](#index)\n",
    "\n",
    "- Acá modificamos el schedule_interval con la nueva sintaxis:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"orquestacion_cron\",\n",
    "    description=\"orquestación usando cron sintax\",\n",
    "    schedule_interval=\"0 7 * * 1\", #Cada \"Lunes a las 7 am\" desde 1° Septiembre hasta el 1° de Octubre.\n",
    "    start_date=datetime(2023, 9, 1),\n",
    "    end_date=datetime(2023, 10, 1)\n",
    ") as dag:\n",
    "    t1 = EmptyOperator(task_id=\"tarea_01\")\n",
    "\n",
    "    t2 = EmptyOperator(task_id=\"tarea_02\")\n",
    "\n",
    "    t3 = EmptyOperator(task_id=\"tarea_03\")\n",
    "    \n",
    "    t4 = EmptyOperator(task_id=\"tarea_04\")\n",
    "\n",
    "    t1 >> t2 >> t3 >> t4\n",
    "\n",
    "```\n",
    "![](img_53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0472f",
   "metadata": {},
   "source": [
    "# <a name=\"mark_16\"></a>Monitoring - Fallos:\n",
    "[Index](#index)\n",
    "\n",
    "- Debido a la aparición de \"FALLOS\" es conveniente realizar el monitoreo de nuestras ejecuciones.\n",
    "- Ejemplos:\n",
    "![](img_54.png)\n",
    "\n",
    "- No Aportó mucho la clase, solo mostró como se ven los fallos en la UI (rojos) y nada más.\n",
    "\n",
    "Ejemplo de fallo causado intensionalmente:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def my_function():\n",
    "    raise Exception\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"monitoreo\",\n",
    "    description=\"monitoreando el DAG\",\n",
    "    schedule_interval=\"@daily\", \n",
    "    start_date=datetime(2023, 9, 1),\n",
    "    end_date=datetime(2023, 10, 1)\n",
    ") as dag:\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"tarea_01\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_01'\")\n",
    "\n",
    "    t2 = BashOperator(\n",
    "        task_id=\"tarea_02\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_02'\")\n",
    "\n",
    "    t3 = BashOperator(\n",
    "        task_id=\"tarea_03\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_03'\")\n",
    "    \n",
    "    t4 = PythonOperator(\n",
    "        task_id=\"tarea_04\",\n",
    "        python_callable=my_function) #Fallará en esta tarea.\n",
    "\n",
    "    t5 = BashOperator(\n",
    "        task_id=\"tarea_05\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_05'\")\n",
    "\n",
    "    t1 >> t2 >> t3 >> t4 >> t5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b7e1bc",
   "metadata": {},
   "source": [
    "# <a name=\"mark_17\"></a>Task Actions, \"Clean\", \"Mark Failed\", y \"Mark Success\":\n",
    "[Index](#index)\n",
    "\n",
    "\n",
    "- Acciones que podemos tomar sobre una tarea.\n",
    "![](img_57.png)\n",
    "\n",
    "- Estados de una tarea:\n",
    "![](img_55.png)\n",
    "\n",
    "- Colores indicativos:\n",
    "![](img_56.png)\n",
    "\n",
    "- \"upstream_failed\": Si la tarea padre falla.\n",
    "\n",
    "## Limpiando tareas con Clean:\n",
    "\n",
    "- selecciono el cuadradito de la tarea + Past + Clean --> Limpia las \"mismas\" tareas, pero anteriores a la seleccionada (en horizontal). **Backfill**\n",
    "- selecciono el cuadradito de la tarea + Future + Clean --> Limpia las \"mismas\" tareas, peor posteriores a la seleccionada (en horizontal).\n",
    "- selecciono el cuadradito de la tarea + upstream + Clean --> Limpia las tareas previas, del mismo día (en vertical).\n",
    "![](img_58.png)\n",
    "- selecciono el cuadradito de la tarea + downstream + Clean --> Contrario a upstream.\n",
    "\n",
    "## Mark Failed:\n",
    "Es una funcionalidad que debemos utilizar si queremos por algun motivo hacer fallar una tarea en particulas y tiene las mismas features \"Past\", \"Future\", \"upstream\", y \"downstream\".\n",
    "\n",
    "## Mark Success:\n",
    "Es lo contrario a Failed y es una analogía a \"pass\" en python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d91c2",
   "metadata": {},
   "source": [
    "# <a name=\"mark_18\"></a>Trigger Rules:\n",
    "[Index](#index)\n",
    "\n",
    "- \"all_success\" Lógica, **Task A \"OK\" & Task B \"OK\" --> Ejecutar Task C**. Por defecto en Airflow\n",
    "- \"all_failed\" Lógica, **Task A \"Faild\" & Task B \"Faild\" --> Ejecutar Task C**.\n",
    "![](img_59.png)\n",
    "- \"all_done\" Lógica, **No importa el estado final, si la tarea terminó --> Ejecutar Task D**.\n",
    "- \"one_success\" Lógica, **Aunque una sola tarea sea success --> Ejecutar Task D**.\n",
    "![](img_60.png)\n",
    "- \"one_failed\" Lógica, **Aunque una sola tarea sea Fail --> Ejecutar Task D**.\n",
    "- \"none_failed\" Lógica, **Si Ninguna de las tareas a Fallado --> Ejecutar Task D**.\n",
    "![](img_61.png)\n",
    "\n",
    "- Ejemplo:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.Python import PythonOperator\n",
    "from datetime import datetime\n",
    "from airflows.utils.trigger_rule import TriggerRule\n",
    "\n",
    "def my_function():\n",
    "    raise Exception\n",
    "\n",
    "default_args = {}#el parámetro de este dict afecta a todas las tareas.\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"6.2-monitoring\",\n",
    "    description=\"monitoreando con Triggers\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2023, 9, 1),\n",
    "    end_date=datetime(2023, 10, 1),\n",
    "    default_args=default_args,\n",
    "    max_active_runs=1\n",
    ") as dag:\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"tarea_01\",\n",
    "        bash_command=\"sleep 5 && echo 'tarea_01'\",\n",
    "        trigger_rule=TriggerRule.ALL_SUCCESS,#en este caso no hace mucho ya que es la primer tarea.\n",
    "        retries=2,#Cantidad de re-ejecuciones.\n",
    "        retry_delay=5,#segundos entre cada re-ejecución.\n",
    "        depends_on_past=False#depende de la tarea anterior (horizontalmente)\n",
    "    )\n",
    "    t2 = BashOperator(\n",
    "        task_id=\"tarea_02\",\n",
    "        bash_command=\"sleep 3 && echo 'tarea_02'\",\n",
    "        retries=2,\n",
    "        retry_delay=5,\n",
    "        trigger_rule=TriggerRule.ALL_SUCCESS,\n",
    "        depends_on_past=True        \n",
    "    )\n",
    "    t3 = BashOperator(\n",
    "        task_id=\"tarea_03\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_03'\",\n",
    "        retries=2,\n",
    "        retry_delay=5,\n",
    "        trigger_rule=TriggerRule.ALWAYS,\n",
    "        depends_on_past=True        \n",
    "    )\n",
    "    t4 = PythonOperator(\n",
    "        task_id=\"tarea_04\",\n",
    "        python_callable=my_function,\n",
    "        retries=2,\n",
    "        retry_delay=5,\n",
    "        trigger_rule=TriggerRule.ALL_SUCCESS,\n",
    "        depends_on_past=True        \n",
    "    )\n",
    "    t5 = BashOperator(\n",
    "        task_id=\"tarea_05\",\n",
    "        bash_command=\"sleep 2 && echo 'tarea_05'\",\n",
    "        retries=2,\n",
    "        retry_delay=5,\n",
    "        trigger_rule=TriggerRule.ALL_SUCCESS,\n",
    "        depends_on_past=True        \n",
    "    )\n",
    "\n",
    "    t1 >> t2 >> t3 >> t4 >> t5 \n",
    "```\n",
    "![](img_62.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae2894",
   "metadata": {},
   "source": [
    "# <a name=\"mark_19\"></a>Qué son los sensores?:\n",
    "[Index](#index)\n",
    "\n",
    "- Documentación:\n",
    "    - https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html\n",
    "    - https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.html#module-airflow.sensors\n",
    "\n",
    "![](img_63.png)\n",
    "\n",
    "## Tipos de sensores.\n",
    "![](img_64.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf0d06",
   "metadata": {},
   "source": [
    "# <a name=\"mark_20\"></a>External Task Sensor\n",
    "[Index](#index)\n",
    "\n",
    "Un ejemplo de caso de uso, tienes un DAG que hace las extracciones, cuando estas terminan, tienes un segundo DAG que realiza las transformaciones.\n",
    "\n",
    "- Primer DAG:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"external_task_sensor_primario\",\n",
    "    description=\"DAG primario\"\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2023, 9, 1),\n",
    "    end_date=datetime(2023, 10, 1)\n",
    ") as dag:\n",
    "\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"tarea_01\",\n",
    "        bash_command=\"sleep 10 && echo 'DAG finalizado'\",\n",
    "        depends_on_past=True\n",
    "    )\n",
    "    t1\n",
    "```\n",
    "![](img_65.png)\n",
    "\n",
    "- Segundo DAG:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "\n",
    "with DAG(\n",
    "    ag_id=\"7.2-external_task_sensor\",\n",
    "    description=\"DAG secundario, se ejecuta cuando termina DAG primario\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2023, 9, 1),\n",
    "    end_date=datetime(2023, 10, 1),\n",
    "    max_active_runs=1\n",
    ") as dag:\n",
    "\n",
    "    t1 = ExternalTaskSensor(\n",
    "        task_id=\"esperando_por_DAG_primario\",\n",
    "        external_dag_id=\"external_task_sensor_primario\",#DAG al que se espera.\n",
    "        '''también se puede uasr el (external_task_id:\"tarea_01\"), donde le pasamos el nombre de la tarea'''\n",
    "        poke_interval=10,#cada 10 seg, el sensor estará preguntando si el proceso ya finalizó, podría reemplazarce con re.scheduler.\n",
    "    )\n",
    "    t2 = BashOperator(\n",
    "        task_id=\"tarea_02\",\n",
    "        bash_command=\"sleep 10 && echo'DAG 2 finalizado'\",\n",
    "        depends_on_past=True\n",
    "    )\n",
    "    t1 >> t2\n",
    "```\n",
    "![](img_66.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee5b432",
   "metadata": {},
   "source": [
    "# <a name=\"mark_21\"></a>File Sensor\n",
    "[Index](#index)\n",
    "\n",
    "- FileSensor necesita por defecto una conexión.\n",
    "![](img_67.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "- Creando la conexión.\n",
    "![](img_68.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "![](img_69.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "![](img_70.png)\n",
    "\n",
    "- Nota: con los parámetros \"Connection Id\" y \"Connection Type\", es suficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be802b9b",
   "metadata": {},
   "source": [
    "# <a name=\"mark_22\"></a>Qué son los templates con Jinja?\n",
    "[Index](#index)\n",
    "\n",
    "- Documentación:\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html\n",
    "\n",
    "![](img_71.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "- Ejemplo:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "#descripción de la acción en Jinja, \"ds\" es el logical date\n",
    "templated_command = '''\n",
    "{% for file in params.filenames %}\n",
    "    echo \"{{ds}}\"\n",
    "    echo \"{{file}}\"\n",
    "{% endfor%}\n",
    "'''\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"8-templating\",\n",
    "    description=\"utilizando un template\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2023, 9, 1),\n",
    "    end_date=datetime(2023, 10, 1),\n",
    "    max_active_runs=1\n",
    ") as dag:\n",
    "\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"tarea_01\",\n",
    "        bash_command=templated_command,\n",
    "        params={\"filenames\":[\"primer impresion\", \"segunda impresion\"]},\n",
    "        depends_on_past=True\n",
    "    )\n",
    "```\n",
    "![](img_72.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38179b4a",
   "metadata": {},
   "source": [
    "# <a name=\"mark_23\"></a>XComs - cross comunications\n",
    "[Index](#index)\n",
    "\n",
    "- Documentación:\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html\n",
    "\n",
    "![](img_73.png)\n",
    "\n",
    "- Ejemplo:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.models.xcom import XCom\n",
    "\n",
    "def my_function(**context):\n",
    "    print(int(context[ti.xcom_pull(task_id='tarea_02')) - 24])\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"9-XComs\",\n",
    "    descriptions=\"relacionando tareas\",\n",
    "    schedule_interval=\"@once\",\n",
    "    start_date=datetime(2023, 9, 1),\n",
    "    end_date=datetime(2023, 10, 1),\n",
    "    max_active_runs=1\n",
    ") as dag:\n",
    "\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"tarea_01\",\n",
    "        bash_command=\"sleep 5 && echo $((3*8))\"\n",
    "    )\n",
    "\n",
    "#La idea es pasar el resultado de la t1 a la t2\n",
    "    t2 = BashOperator(\n",
    "        task_id=\"tarea_02\",\n",
    "        bash_command=\"sleep 3 && echo {{ ti.xcom_pull(task_ids='tarea_01')}}\",#ti=task instans\n",
    "    )\n",
    "\n",
    "#pasando resultados de t2 a t3 con Python\n",
    "    t3 = PythonOperator(\n",
    "        task_id=\"tarea_03\",\n",
    "        python_callable=my_function\n",
    "    )\n",
    "    t1 >> t2 >> t3\n",
    "```\n",
    "![](img_74.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb7e7a",
   "metadata": {},
   "source": [
    "# <a name=\"mark_24\"></a>Branch Python Operator\n",
    "[Index](#index)\n",
    "\n",
    "- Es un escenario donde nos interesa que nuestro DAG tome un camino u otro dependiendo se la condición se cumple o no. \n",
    "![](img_75.png)\n",
    "\n",
    "- Ejemplo:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "\n",
    "default_args={\n",
    "    start_date=datetime(2022, 7, 1),\n",
    "    end_date=datetime(2022, 8, 1)\n",
    "}\n",
    "\n",
    "def _choose(**context):\n",
    "    #logical_date viene en timestamp la pasamos a date.\n",
    "    if context[\"logical_date\"].date() < date(2022, 7, 15):\n",
    "        return \"finish_14_june\"\n",
    "    \n",
    "    return \"start_15_june\"\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"10-barnch_python_operator\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    default_args=default_args\n",
    ") as dag:\n",
    "\n",
    "    #branching tendrá la lógica para la toma de desición.\n",
    "    \n",
    "    branching = BranchPythonOperator(\n",
    "        task_id=\"branching\",\n",
    "        python_callable=_choose\n",
    "    )\n",
    "    finish_14 = BashOperator(\n",
    "        task_id=\"finish_14_june\",\n",
    "        bash_command=\"echo 'Running {{ds}}'\"\n",
    "    )\n",
    "    start_15 = BashOperator(\n",
    "        task_id=\"start_15_june\",\n",
    "        bash_command=\"echo 'Running {{ds}}\"\n",
    "    )\n",
    "\n",
    "    branching >> [finish_14, start_15]\n",
    "```\n",
    "![](img_77.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "- Lógica ejecutada:\n",
    "![](img_76.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "![](img_78.png)\n",
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "- Nota: \n",
    "**Falto una cosa que nos puede pasar y no entender el porque si existiese otra tarea después de start_15_june dependiendo el flujo si estas son skiped estas pueden no ejecutarse y también saltarse para evitar eso debemos usar el parámetro trigger_rule=TriggerRule.NONE_FAILED, El objeto TriggerRule se importa así**\n",
    "\n",
    "```python\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d313e",
   "metadata": {},
   "source": [
    "# <a name=\"mark_25\"></a>\n",
    "[Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04264db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc36f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e520fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel_01",
   "language": "python",
   "name": "data_trans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
